import asyncio
import logging
import json
import time
import os
from datetime import datetime
from typing import Dict, List, Optional, Any
from playwright.async_api import async_playwright, Browser, BrowserContext, Page, TimeoutError
from dataclasses import dataclass, field

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
def setup_logging():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–∏—Å—Ç–µ–º—ã –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –ª–æ–≥–æ–≤ –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
    if not os.path.exists('logs'):
        os.makedirs('logs')
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    date_format = '%Y-%m-%d %H:%M:%S'
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –ª–æ–≥–≥–µ—Ä–∞
    logging.basicConfig(
        level=logging.INFO,
        format=log_format,
        datefmt=date_format,
        handlers=[
            logging.FileHandler('logs/facebook_scraper.log', encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
    # –û—Ç–¥–µ–ª—å–Ω—ã–π –ª–æ–≥–≥–µ—Ä –¥–ª—è –∫—É–∫–∏
    cookie_logger = logging.getLogger('cookies')
    cookie_handler = logging.FileHandler('logs/cookies.log', encoding='utf-8')
    cookie_handler.setFormatter(logging.Formatter(log_format, date_format))
    cookie_logger.addHandler(cookie_handler)
    cookie_logger.setLevel(logging.INFO)
    
    # –û—Ç–¥–µ–ª—å–Ω—ã–π –ª–æ–≥–≥–µ—Ä –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞
    scraper_logger = logging.getLogger('scraper')
    scraper_handler = logging.FileHandler('logs/scraper_activity.log', encoding='utf-8')
    scraper_handler.setFormatter(logging.Formatter(log_format, date_format))
    scraper_logger.addHandler(scraper_handler)
    scraper_logger.setLevel(logging.INFO)
    
    # –õ–æ–≥–≥–µ—Ä –¥–ª—è –æ—à–∏–±–æ–∫
    error_logger = logging.getLogger('errors')
    error_handler = logging.FileHandler('logs/errors.log', encoding='utf-8')
    error_handler.setFormatter(logging.Formatter(log_format, date_format))
    error_logger.addHandler(error_handler)
    error_logger.setLevel(logging.ERROR)
    
    return {
        'main': logging.getLogger(__name__),
        'cookies': cookie_logger,
        'scraper': scraper_logger,
        'errors': error_logger
    }

@dataclass
class Comment:
    author: str
    text: str
    timestamp: str
    likes: int = 0
    replies: List['Comment'] = field(default_factory=list)

@dataclass
class Post:
    author: str
    text: str
    timestamp: str
    likes: int = 0
    comments_count: int = 0
    shares: int = 0
    reactions: Dict[str, int] = field(default_factory=dict)
    comments: List[Comment] = field(default_factory=list)

class CookieManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∫—É–∫–∏"""
    
    def __init__(self, cookies_file: str = "facebook_cookies.json"):
        self.cookies_file = cookies_file
        self.logger = logging.getLogger('cookies')
        
    async def save_cookies(self, context: BrowserContext) -> bool:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫—É–∫–∏ –≤ —Ñ–∞–π–ª"""
        try:
            cookies = await context.cookies()
            
            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
            cookies_dir = os.path.dirname(self.cookies_file) if os.path.dirname(self.cookies_file) else '.'
            if not os.path.exists(cookies_dir):
                os.makedirs(cookies_dir)
            
            with open(self.cookies_file, 'w', encoding='utf-8') as f:
                json.dump(cookies, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"‚úÖ –ö—É–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {self.cookies_file} (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ: {len(cookies)})")
            
            # –õ–æ–≥–∏—Ä—É–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –∫—É–∫–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            important_cookies = [cookie for cookie in cookies if cookie['name'] in ['c_user', 'xs', 'sb', 'datr']]
            for cookie in important_cookies:
                self.logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫—É–∫–∏: {cookie['name']} –¥–ª—è –¥–æ–º–µ–Ω–∞ {cookie['domain']}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∫—É–∫–∏: {e}")
            return False
    
    async def load_cookies(self, context: BrowserContext) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫—É–∫–∏ –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            if not os.path.exists(self.cookies_file):
                self.logger.warning(f"‚ö†Ô∏è –§–∞–π–ª –∫—É–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {self.cookies_file}")
                return False
            
            with open(self.cookies_file, 'r', encoding='utf-8') as f:
                cookies = json.load(f)
            
            if not cookies:
                self.logger.warning("‚ö†Ô∏è –§–∞–π–ª –∫—É–∫–∏ –ø—É—Å—Ç")
                return False
            
            await context.add_cookies(cookies)
            self.logger.info(f"‚úÖ –ö—É–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ —Ñ–∞–π–ª–∞: {self.cookies_file} (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ: {len(cookies)})")
            
            # –õ–æ–≥–∏—Ä—É–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –∫—É–∫–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            important_cookies = [cookie for cookie in cookies if cookie['name'] in ['c_user', 'xs', 'sb', 'datr']]
            for cookie in important_cookies:
                self.logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–∞ –∫—É–∫–∏: {cookie['name']} –¥–ª—è –¥–æ–º–µ–Ω–∞ {cookie['domain']}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∫—É–∫–∏: {e}")
            return False
    
    def cookies_exist(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞ –∫—É–∫–∏"""
        exists = os.path.exists(self.cookies_file)
        self.logger.info(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—É–∫–∏ —Ñ–∞–π–ª–∞ {self.cookies_file}: {'—Å—É—â–µ—Å—Ç–≤—É–µ—Ç' if exists else '–Ω–µ –Ω–∞–π–¥–µ–Ω'}")
        return exists
    
    async def validate_cookies(self, page: Page) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ –∫—É–∫–∏ —á–µ—Ä–µ–∑ –ø–µ—Ä–µ—Ö–æ–¥ –Ω–∞ Facebook"""
        try:
            self.logger.info("üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –∫—É–∫–∏...")
            
            await page.goto('https://www.facebook.com', wait_until='networkidle')
            await page.wait_for_timeout(3000)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω—ã –ª–∏ –º—ã
            login_indicators = [
                'input[name="email"]',  # –ü–æ–ª–µ –≤–≤–æ–¥–∞ email –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –≤—Ö–æ–¥–∞
                'input[data-testid="royal_email"]'  # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–µ –ø–æ–ª–µ
            ]
            
            is_logged_out = False
            for indicator in login_indicators:
                if await page.query_selector(indicator):
                    is_logged_out = True
                    break
            
            if is_logged_out:
                self.logger.warning("‚ö†Ô∏è –ö—É–∫–∏ –Ω–µ–≤–∞–ª–∏–¥–Ω—ã - —Ç—Ä–µ–±—É–µ—Ç—Å—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è")
                return False
            else:
                self.logger.info("‚úÖ –ö—É–∫–∏ –≤–∞–ª–∏–¥–Ω—ã - –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω")
                return True
                
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –∫—É–∫–∏: {e}")
            return False

class FacebookDOMAnalyzer:
    def __init__(self):
        self.logger = logging.getLogger('scraper')
    
    async def get_selectors(self, selector_type: str) -> Dict[str, str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ Facebook"""
        selectors = {
            'post': {
                'container': '[data-pagelet="FeedUnit_0"], [role="article"], div[data-testid="fbfeed_story"]',
                'author': '[data-testid="story-subtitle"] a, [data-ad-comet-preview="message"] a',
                'content': '[data-testid="post_message"], [data-ad-comet-preview="message"]',
                'timestamp': 'abbr[data-utime], time',
                'likes': '[aria-label*="reactions"], [aria-label*="–ª–∞–π–∫"]',
                'comments': '[aria-label*="comment"], [aria-label*="–∫–æ–º–º–µ–Ω—Ç"]',
                'shares': '[aria-label*="share"], [aria-label*="–ø–æ–¥–µ–ª–∏–ª"]'
            },
            'comment': {
                'container': '[role="article"], div[aria-label*="Comment"], div[data-testid="UFI2Comment/root"]',
                'author': 'strong a, [data-testid="UFI2CommentActorName"] a, h3 a',
                'text': '[data-testid="UFI2CommentBodyText"], div[dir="auto"]:not([role="button"])',
                'timestamp': 'abbr, time, a[role="link"][tabindex="0"]',
                'likes': '[aria-label*="reactions"], button[aria-label*="–ª–∞–π–∫"], [aria-label*="Like"]',
                'replies': '[aria-label*="replies"], [aria-label*="–æ—Ç–≤–µ—Ç"]'
            },
            'modal': {
                'container': '[role="dialog"], div[aria-modal="true"]',
                'close': '[aria-label="Close"], button[aria-label="–ó–∞–∫—Ä—ã—Ç—å"], [aria-label*="close"]',
                'comments_section': '[aria-label*="Comments"], div[data-testid="UFI2CommentsCount"]',
                'content': '[role="dialog"] [data-testid="post_message"], div[aria-modal="true"] [data-ad-comet-preview="message"]'
            }
        }
        return selectors.get(selector_type, {})

    async def analyze_modal(self, page: Page) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–æ–¥–∞–ª—å–Ω–æ–µ –æ–∫–Ω–æ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –µ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º"""
        try:
            modal_selector = '[role="dialog"], div[aria-modal="true"]'
            modal = await page.query_selector(modal_selector)
            
            if not modal:
                return {"is_modal": False, "type": None}
            
            modal_type = "unknown"
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø –º–æ–¥–∞–ª—å–Ω–æ–≥–æ –æ–∫–Ω–∞
            comments_indicators = [
                '[aria-label*="Comments"]',
                'div[data-testid="UFI2CommentsCount"]',
                'h2:has-text("Comments")',
                'h2:has-text("–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏")',
                '[data-testid="UFI2Comment/root"]'
            ]
            
            for indicator in comments_indicators:
                if await page.query_selector(indicator):
                    modal_type = "comments"
                    break
            
            media_indicators = [
                'img[data-testid="photo"]',
                'video',
                '[aria-label*="Photo"]',
                '[aria-label*="Video"]',
                'div[data-testid="media-viewer"]'
            ]
            
            for indicator in media_indicators:
                if await page.query_selector(indicator):
                    modal_type = "media" if modal_type == "unknown" else f"{modal_type}_media"
                    break
            
            has_comments_section = any(await modal.query_selector(indicator) for indicator in comments_indicators)
            
            self.logger.info(f"üì± –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –º–æ–¥–∞–ª—å–Ω–æ–µ –æ–∫–Ω–æ —Ç–∏–ø–∞: {modal_type}")
            
            return {
                "is_modal": True,
                "type": modal_type,
                "has_comments": has_comments_section,
                "modal_element": modal,
                "selectors": await self.get_selectors('modal')
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –º–æ–¥–∞–ª—å–Ω–æ–≥–æ –æ–∫–Ω–∞: {e}")
            logging.getLogger('errors').error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –º–æ–¥–∞–ª—å–Ω–æ–≥–æ –æ–∫–Ω–∞: {e}")
            return {"is_modal": False, "type": None, "error": str(e)}

class FacebookScraper:
    def __init__(self, headless: bool = True, cookies_file: str = "cookies.json"):
        self.headless = headless
        self.browser: Optional[Browser] = None
        self.context: Optional[BrowserContext] = None
        self.page: Optional[Page] = None
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        self.loggers = setup_logging()
        self.logger = self.loggers['main']
        self.scraper_logger = self.loggers['scraper']
        self.error_logger = self.loggers['errors']
        
        # –ú–µ–Ω–µ–¥–∂–µ—Ä—ã
        self.cookie_manager = CookieManager(cookies_file)
        self.dom_analyzer = FacebookDOMAnalyzer()
        
        self.logger.info("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è FacebookScraper")

    async def start_browser(self):
        """–ó–∞–ø—É—Å–∫ –±—Ä–∞—É–∑–µ—Ä–∞ —Å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ–º —Å–µ—Å—Å–∏–∏"""
        try:
            self.logger.info("üåê –ó–∞–ø—É—Å–∫ –±—Ä–∞—É–∑–µ—Ä–∞...")
            
            self.playwright = await async_playwright().start()
            self.browser = await self.playwright.chromium.launch(
                headless=self.headless,
                args=[
                    '--no-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-blink-features=AutomationControlled',
                    '--disable-web-security',
                    '--disable-features=VizDisplayCompositor'
                ]
            )
            
            self.context = await self.browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            )
            
            self.page = await self.context.new_page()
            
            # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –∫—É–∫–∏
            if self.cookie_manager.cookies_exist():
                if await self.cookie_manager.load_cookies(self.context):
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –∫—É–∫–∏
                    if await self.cookie_manager.validate_cookies(self.page):
                        self.logger.info("‚úÖ –°–µ—Å—Å–∏—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –∏–∑ –∫—É–∫–∏")
                        return True
                    else:
                        self.logger.warning("‚ö†Ô∏è –ö—É–∫–∏ —É—Å—Ç–∞—Ä–µ–ª–∏, —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–æ–≤—Ç–æ—Ä–Ω–∞—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è")
            
            self.logger.info("‚úÖ –ë—Ä–∞—É–∑–µ—Ä —É—Å–ø–µ—à–Ω–æ –∑–∞–ø—É—â–µ–Ω")
            return True
            
        except Exception as e:
            self.error_logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –±—Ä–∞—É–∑–µ—Ä–∞: {e}")
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –±—Ä–∞—É–∑–µ—Ä–∞: {e}")
            raise

    async def close_browser(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ –±—Ä–∞—É–∑–µ—Ä–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å–µ—Å—Å–∏–∏"""
        try:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫—É–∫–∏ –ø–µ—Ä–µ–¥ –∑–∞–∫—Ä—ã—Ç–∏–µ–º
            if self.context:
                await self.cookie_manager.save_cookies(self.context)
            
            if self.page:
                await self.page.close()
            if self.context:
                await self.context.close()
            if self.browser:
                await self.browser.close()
            if hasattr(self, 'playwright'):
                await self.playwright.stop()
                
            self.logger.info("‚úÖ –ë—Ä–∞—É–∑–µ—Ä –∑–∞–∫—Ä—ã—Ç, —Å–µ—Å—Å–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
            
        except Exception as e:
            self.error_logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏ –±—Ä–∞—É–∑–µ—Ä–∞: {e}")
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏ –±—Ä–∞—É–∑–µ—Ä–∞: {e}")

    async def login(self, email: str, password: str) -> bool:
        """–ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –≤ Facebook —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å–µ—Å—Å–∏–∏"""
        try:
            self.logger.info(f"üîê –ù–∞—á–∏–Ω–∞–µ–º –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—é –¥–ª—è {email}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω—ã –ª–∏ –º—ã —É–∂–µ
            await self.page.goto('https://www.facebook.com', wait_until='networkidle')
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å –ø–æ–ª–µ –¥–ª—è –≤—Ö–æ–¥–∞, –∑–Ω–∞—á–∏—Ç –Ω–µ –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω—ã
            email_field = await self.page.query_selector('input[name="email"]')
            if not email_field:
                self.logger.info("‚úÖ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —É–∂–µ –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω")
                return True
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—é
            self.logger.info("üìù –í–≤–æ–¥–∏–º —É—á–µ—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
            await self.page.fill('input[name="email"]', email)
            await self.page.fill('input[name="pass"]', password)
            
            # –ù–∞–∂–∏–º–∞–µ–º –∫–Ω–æ–ø–∫—É –≤—Ö–æ–¥–∞
            await self.page.click('button[name="login"]')
            await self.page.wait_for_load_state('networkidle')
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å–ø–µ—à–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–∞
            await self.page.wait_for_timeout(3000)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —É—Å–ø–µ—à–Ω–æ–≥–æ –≤—Ö–æ–¥–∞
            success_indicators = [
                # –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —Ñ–æ—Ä–º—ã –≤—Ö–æ–¥–∞
                lambda: self.page.query_selector('input[name="email"]') is None,
                # –ù–∞–ª–∏—á–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                lambda: self.page.query_selector('[aria-label*="Account"]') is not None,
                lambda: self.page.query_selector('[data-testid="blue_bar"]') is not None,
            ]
            
            is_logged_in = False
            for check in success_indicators:
                try:
                    if await check():
                        is_logged_in = True
                        break
                except:
                    continue
            
            if is_logged_in and 'facebook.com' in self.page.url and 'login' not in self.page.url:
                self.logger.info("‚úÖ –£—Å–ø–µ—à–Ω–∞—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è")
                self.scraper_logger.info(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å {email} —É—Å–ø–µ—à–Ω–æ –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω")
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫—É–∫–∏ –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–π –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
                await self.cookie_manager.save_cookies(self.context)
                return True
            else:
                self.logger.error("‚ùå –û—à–∏–±–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏")
                self.error_logger.error(f"–ù–µ—É–¥–∞—á–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è {email}")
                return False
                
        except Exception as e:
            self.error_logger.error(f"–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ {email}: {e}")
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏: {e}")
            return False

    async def extract_full_comments(self, page: Page, modal: bool = False) -> List[Comment]:
        """–ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ –∏–∑ –º–æ–¥–∞–ª—å–Ω–æ–≥–æ –æ–∫–Ω–∞"""
        comments = []
        try:
            self.scraper_logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ ({'–º–æ–¥–∞–ª—å–Ω–æ–µ –æ–∫–Ω–æ' if modal else '—Å—Ç—Ä–∞–Ω–∏—Ü–∞'})")
            print(f"\033[94m–ò—â–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ ({'–≤ –º–æ–¥–∞–ª—å–Ω–æ–º –æ–∫–Ω–µ' if modal else '–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ'})...\033[0m")
            
            comment_selectors = await self.dom_analyzer.get_selectors('comment')
            
            if modal:
                comment_containers = await page.query_selector_all(
                    f'[role="dialog"] {comment_selectors["container"]}, '
                    f'div[aria-modal="true"] {comment_selectors["container"]}'
                )
            else:
                comment_containers = await page.query_selector_all(comment_selectors['container'])
            
            self.scraper_logger.info(f"–ù–∞–π–¥–µ–Ω–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤: {len(comment_containers)}")
            print(f"\033[92m–ù–∞–π–¥–µ–Ω–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤: {len(comment_containers)}\033[0m")
            
            for i, container in enumerate(comment_containers):
                try:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∞–≤—Ç–æ—Ä–∞
                    author_element = await container.query_selector(comment_selectors['author'])
                    author = await author_element.inner_text() if author_element else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∞–≤—Ç–æ—Ä"
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è
                    text_element = await container.query_selector(comment_selectors['text'])
                    text = await text_element.inner_text() if text_element else ""
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –º–µ—Ç–∫—É
                    timestamp_element = await container.query_selector(comment_selectors['timestamp'])
                    timestamp = ""
                    if timestamp_element:
                        timestamp = await timestamp_element.get_attribute('data-utime')
                        if not timestamp:
                            timestamp = await timestamp_element.inner_text()
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∞–π–∫–æ–≤
                    likes = 0
                    likes_element = await container.query_selector(comment_selectors['likes'])
                    if likes_element:
                        likes_text = await likes_element.get_attribute('aria-label') or ""
                        import re
                        likes_match = re.search(r'(\d+)', likes_text)
                        if likes_match:
                            likes = int(likes_match.group(1))
                    
                    if text.strip():
                        comment = Comment(
                            author=author.strip(),
                            text=text.strip(),
                            timestamp=timestamp,
                            likes=likes
                        )
                        comments.append(comment)
                        
                        self.scraper_logger.info(f"–ò–∑–≤–ª–µ—á–µ–Ω –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –æ—Ç {author[:30]}: {text[:50]}...")
                        print(f"\033[93m–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π {i+1}: {author[:30]}... | {text[:50]}...\033[0m")
                
                except Exception as e:
                    self.error_logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è {i+1}: {e}")
                    print(f"\033[91m–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è {i+1}: {e}\033[0m")
                    continue
            
            self.scraper_logger.info(f"–í—Å–µ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–æ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤: {len(comments)}")
            print(f"\033[92m–í—Å–µ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–æ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤: {len(comments)}\033[0m")
            
        except Exception as e:
            self.error_logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤: {e}")
            print(f"\033[91m–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤: {e}\033[0m")
        
        return comments

    async def click_view_more_comments(self, page: Page, max_attempts: int = 5) -> bool:
        """–ù–∞–∂–∏–º–∞–µ–º –Ω–∞ –∫–Ω–æ–ø–∫–∏ '–ü–æ–∫–∞–∑–∞—Ç—å –µ—â–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏'"""
        attempts = 0
        clicked_any = False
        
        view_more_selectors = [
            'div[role="button"]:has-text("View more comments")',
            'div[role="button"]:has-text("–ü–æ–∫–∞–∑–∞—Ç—å –µ—â–µ")',
            'span:has-text("View more comments")',
            'span:has-text("–ü–æ–∫–∞–∑–∞—Ç—å –µ—â–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏")',
            '[data-testid="UFI2CommentsCount/root"] div[role="button"]'
        ]
        
        while attempts < max_attempts:
            found_button = False
            
            for selector in view_more_selectors:
                try:
                    buttons = await page.query_selector_all(selector)
                    for button in buttons:
                        if await button.is_visible():
                            await button.click()
                            await page.wait_for_timeout(2000)
                            clicked_any = True
                            found_button = True
                            
                            self.scraper_logger.info(f"–ù–∞–∂–∞—Ç–∞ –∫–Ω–æ–ø–∫–∞ '–ü–æ–∫–∞–∑–∞—Ç—å –µ—â–µ' (–ø–æ–ø—ã—Ç–∫–∞ {attempts + 1})")
                            print(f"\033[94m‚úì –ù–∞–∂–∞—Ç–∞ –∫–Ω–æ–ø–∫–∞ '–ü–æ–∫–∞–∑–∞—Ç—å –µ—â–µ' (–ø–æ–ø—ã—Ç–∫–∞ {attempts + 1})\033[0m")
                            break
                    if found_button:
                        break
                except Exception as e:
                    continue
            
            if not found_button:
                break
                
            attempts += 1
        
        return clicked_any

    async def scrape_post_comments(self, post_url: str) -> Dict[str, Any]:
        """–°–∫—Ä–∞–ø–∏–Ω–≥ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É –ø–æ—Å—Ç—É"""
        try:
            self.scraper_logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º —Å–∫—Ä–∞–ø–∏–Ω–≥ –ø–æ—Å—Ç–∞: {post_url}")
            print(f"\033[96m=== –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ –ø–æ—Å—Ç—É: {post_url} ===\033[0m")
            
            await self.page.goto(post_url)
            await self.page.wait_for_load_state('networkidle')
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–æ–¥–∞–ª—å–Ω–æ–µ –æ–∫–Ω–æ
            modal_info = await self.dom_analyzer.analyze_modal(self.page)
            is_modal = modal_info.get('is_modal', False)
            
            print(f"\033[93m–†–µ–∂–∏–º: {'–ú–æ–¥–∞–ª—å–Ω–æ–µ –æ–∫–Ω–æ' if is_modal else '–û–±—ã—á–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞'}\033[0m")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–æ–ª—å—à–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
            await self.click_view_more_comments(self.page)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
            comments = await self.extract_full_comments(self.page, modal=is_modal)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ—Å—Ç–µ
            post_selectors = await self.dom_analyzer.get_selectors('post')
            
            author_element = await self.page.query_selector(post_selectors['author'])
            author = await author_element.inner_text() if author_element else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∞–≤—Ç–æ—Ä"
            
            content_element = await self.page.query_selector(post_selectors['content'])
            content = await content_element.inner_text() if content_element else ""
            
            timestamp_element = await self.page.query_selector(post_selectors['timestamp'])
            timestamp = ""
            if timestamp_element:
                timestamp = await timestamp_element.get_attribute('data-utime') or await timestamp_element.inner_text()
            
            post = Post(
                author=author,
                text=content,
                timestamp=timestamp,
                comments=comments,
                comments_count=len(comments)
            )
            
            result = {
                'post': post,
                'url': post_url,
                'scraped_at': datetime.now().isoformat(),
                'total_comments': len(comments),
                'is_modal': is_modal
            }
            
            self.scraper_logger.info(f"–°–∫—Ä–∞–ø–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. URL: {post_url}, –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤: {len(comments)}")
            print(f"\033[92m‚úì –°–∫—Ä–∞–ø–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. –ù–∞–π–¥–µ–Ω–æ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤: {len(comments)}\033[0m")
            
            return result
            
        except Exception as e:
            self.error_logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫—Ä–∞–ø–∏–Ω–≥–µ –ø–æ—Å—Ç–∞ {post_url}: {e}")
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫—Ä–∞–ø–∏–Ω–≥–µ –ø–æ—Å—Ç–∞ {post_url}: {e}")
            return {'error': str(e), 'url': post_url}

    async def save_results(self, results: Dict[str, Any], filename: str = None):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ JSON —Ñ–∞–π–ª"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"results/facebook_comments_{timestamp}.json"
        
        try:
            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
            results_dir = os.path.dirname(filename) if os.path.dirname(filename) else '.'
            if not os.path.exists(results_dir):
                os.makedirs(results_dir)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º dataclass –æ–±—ä–µ–∫—Ç—ã –≤ —Å–ª–æ–≤–∞—Ä–∏
            serializable_results = self._make_serializable(results)
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(serializable_results, f, ensure_ascii=False, indent=2)
            
            self.scraper_logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {filename}")
            print(f"\033[92m‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {filename}\033[0m")
            
        except Exception as e:
            self.error_logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {e}")
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {e}")

    def _make_serializable(self, obj):
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ —Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç"""
        if isinstance(obj, (Post, Comment)):
            return obj.__dict__
        elif isinstance(obj, dict):
            return {k: self._make_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_serializable(item) for item in obj]
        else:
            return obj

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
async def main():
    scraper = FacebookScraper(headless=False, cookies_file="facebook_cookies.json")
    
    try:
        await scraper.start_browser()
        
        # –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è (–∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –≤–∞—à–∏ –¥–∞–Ω–Ω—ã–µ)
        email = " "
        password = " "
        
        if await scraper.login(email, password):
            # URL –ø–æ—Å—Ç–∞ –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞
            post_url = "https://www.facebook.com/groups/1075275215820713"
            
            # –°–∫—Ä–∞–ø–∏–Ω–≥ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
            results = await scraper.scrape_post_comments(post_url)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            await scraper.save_results(results)
            
    except Exception as e:
        logging.getLogger('errors').error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ main: {e}")
        print(f"\033[91m–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}\033[0m")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())
